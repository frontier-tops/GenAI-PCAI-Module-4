{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd03dbf2-7e12-4b6d-b325-2c4f10ea2ab3",
   "metadata": {},
   "source": [
    "# RAG Using Different LLM Endpoints in PCAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc737e3-dc25-4744-9295-7fcd26c2de29",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0197765d-6014-4434-bb59-daf1ea4534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d529650-d181-4dff-a9b5-e2bc2ef62bad",
   "metadata": {},
   "source": [
    "## Fetching the Secret Token for RAG Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a304d1-baba-4586-bd23-bddf732c3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "#getting the auth token\n",
    "secret_file_path = \"/etc/secrets/ezua/.auth_token\"\n",
    "\n",
    "with open(secret_file_path, \"r\") as file:\n",
    "    token = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb97a8d-2021-4ef5-86fd-66f611bdedf1",
   "metadata": {},
   "source": [
    "## Connecting to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263a85b7-022f-43f6-a730-42f73e969422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/weaviate/warnings.py:340: UserWarning: Con006: You're using the sync client in an async context. This usage is discouraged to avoid blocking your async event loop with sync I/O calls.\n",
      "            We encourage you to update your code to use the async client instead when running inside async def functions!\n",
      "  warnings.warn(\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/meta \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "domain = \".cluster.local\"\n",
    "http_host = \"weaviate.hpe-weaviate.svc.cluster.local\"\n",
    "grpc_host = \"weaviate-grpc.hpe-weaviate.svc\" + domain\n",
    "weaviate_headers = {\"x-auth-token\": token}\n",
    "#weaviate_headers = {\"x-auth-token\": \"wrong token\"}\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=http_host,        # Hostname for the HTTP API connection\n",
    "    http_port=80,              # Default is 80, WCD uses 443\n",
    "    http_secure=False,           # Whether to use https (secure) for the HTTP API connection\n",
    "    grpc_host=grpc_host,        # Hostname for the gRPC API connection\n",
    "    grpc_port=50051,              # Default is 50051, WCD uses 443\n",
    "    grpc_secure=False,           # Whether to use a secure channel for the gRPC API connection\n",
    "    headers=weaviate_headers,\n",
    "    skip_init_checks=False\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831d6c5-b101-4d9a-a582-b55fec175991",
   "metadata": {},
   "source": [
    "## Connecting to LLM through MLIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22a5b1b-b534-461c-9a14-93a012db27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:151: UserWarning: https://nvidia-nim-model-predictor-haris-crimsoncl-3444a1bb.pcai1.genai1.hou does not end in /v1, you may have inference and listing issues\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_api_key = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDgzNDIxNzEsImlhdCI6MTc0MzE1ODE3NCwiaXNzIjoiYWlvbGlAaHBlLmNvbSIsInN1YiI6IjAwN2ExM2Q3LWM3ZGYtNDkxNy1iODNiLTc5NjkyMzUxYzk4OCIsInVzZXIiOiJoYXJpcy1jcmltc29uY2xvdWQuaW4ifQ.PkuygOyMTg3vHwIUGMfn6ymca4eEBWCQbA9b4xPJ1O27egSP0LVAOPhIG_B4NxKs3wq3r-Xfl-ZVyAGh_ihTSvjjHbondTNTW1EJK5lMXRQ1cWYw4xCLakwHpCGR4-MBz6UgTTZx5mSHvd_X_ZR9wsPV4ab532q1j1JKnGSNz_oVvoDLfghmLsjg3fo7Qr1sd81fra39C0bMtUl55EpcfKkjwR092XO3q1JTjOu1ZQaar7yRGrI7TN63A534xfX6dpWbKx0goDOwP4lBpduzrUnbkxFQeJdIZFFKJDXvPCMgMl2n2rDgOmZ9cC_PYfWMh8Jvh-iZ4wronY6eTcuj9g\" \n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    base_url=\"https://nvidia-nim-model-predictor-haris-crimsoncl-3444a1bb.pcai1.genai1.hou\",\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    api_key=_api_key,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d1851-c89b-4a96-908d-73978a46498a",
   "metadata": {},
   "source": [
    "## Data Extraction and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74087bad-8c82-4c54-bbd3-033e77ffaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your PDF\n",
    "pdf_path = \"docs/HPE.pdf\"\n",
    "\n",
    "# Load PDF file\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into manageable chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8b83e-3e1b-48c2-99e3-1ba7d4fab411",
   "metadata": {},
   "source": [
    "## Vector Store Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22599776-f7c7-47fc-84f9-8946f8da9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/RAG \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/RAG \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://ollama.pcai1.genai1.hou/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "vector = WeaviateVectorStore.from_documents(docs, embedding=OllamaEmbeddings(model = \"nomic-embed-text:latest\", base_url=\"https://ollama.pcai1.genai1.hou\"), client=client, index_name=\"RAG\", text_key=\"Rag\".lower() + \"_key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eedbd2-d552-473f-b4b3-69adb44f38c2",
   "metadata": {},
   "source": [
    "## Retriever Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73394d2-b53b-414c-b0dc-a0d86327a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b221ae9-07f6-462c-a916-e83d01b511a8",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2d767-9425-461c-9b79-27af7ad4bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v1\",\n",
    "                          base_url=\"https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou\",\n",
    "                          api_key=token)\n",
    "\n",
    "compressor.get_available_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66049a-8cba-4b4a-a09d-cdbdf7e120e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786bc9-a8a3-49c9-b57a-effdd202942b",
   "metadata": {},
   "source": [
    "## User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a345a03-46b7-4c5d-8dba-86d8ae0f0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is HPE Proliant Compute DL384 Gen12\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c476-1e68-4797-a428-62a3893e1c4d",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d6a4fc-832c-4e02-91aa-a174d8212e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://ollama.pcai1.genai1.hou/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is HPE Proliant Compute DL384 Gen12',\n",
       " 'result': 'According to the provided QuickSpecs document, the HPE ProLiant Compute DL384 Gen12 is a 2U standard 19” rack design, air-cooled server. It is a type of rack mount server manufactured by Hewlett Packard Enterprise (HPE).'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811baea8-8a0a-4479-83cd-54eed4bba475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86d277-b91d-4629-84b9-42bf215687b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
